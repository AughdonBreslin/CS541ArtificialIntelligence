# 3. Gradient Calculation

Suppose **x** and y are known, $\bold{w} \in \mathbb{R}^d$ is a column vector. Consider the following functions that have been broadly used in machine learning.

- Sigmoid: $F(\bold{w})=\frac{1}{1+e^{-\bold{x} \cdot \bold{w}}}$
- Hinge Loss: $F(\bold{w})=max(0, 1-y\bold{x} \cdot \bold{w})$
- $\ell_1$-norm: $F(\bold{w})=||\bold{w}||_1$

1. Use python to plot their curves for the case d = 1. You can set x = y = 1.
   <img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221018123032564.png" style="zoom:50%;" /><img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221018123041986.png" alt="image-20221018123041986" style="zoom:50%;" /><img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221018123047234.png" alt="image-20221018123047234" style="zoom:50%;" />
2. Derive their gradient or subgradients for a general $d>0$. 
   To get the gradient, we differentiate the loss with respect to the $i^{th}$ component of $w$.
   - Sigmoid
     - $\frac{\partial F}{\partial \bold{w}} = \frac{\bold{x}e^{-\bold{x} \cdot \bold{w}}}{(1+e^{-\bold{x} \cdot \bold{w}})^2}$
   - Hinge
     - $\frac{\partial F}{\partial \bold{w}}=\begin{cases}
       -y\ \bold{x} &\text{if } y\ \mathbf{x}\cdot \mathbf{w} < 1 \\\
       0&\text{if } y\ \mathbf{x}\cdot \mathbf{w} > 1 \end{cases}$
   - $\ell_1$-norm
     - $\frac{\partial F}{\partial \bold{w}}=\frac{\bold{w}}{|\bold{w}|}$

# 3.1 Implementation

Gradient descent is typically used to solve a general optimization problem
$$
min_{\bold{w} \in \mathbb{R}^d} F(\bold{w}). \qquad \qquad (6.4)
$$
It starts from an arbitrary point $\bold{w}^0$ and gradually refines the solution as
$$
\bold{w}^t \leftarrow \bold{w}^{t-1}-\eta \cdot \nabla F(\bold{w}^{t-1}).
$$
Fix d = 1, i.e., the variable $w$ is scalar. Further fix $w^0=1$.

1. Consider $F(w)=\frac{1}{2}w^2$. For each learning rate $\eta \in \{10^{-4},10^{-3},0.01,0.1,0.5,1,2,5,10,100\}$, calculate the sequence $\{w^t\}_{t=1}^{1000}$ generated by GD and plot the curve "$|w^t|$ v.s. $t$".
   - <img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221019194701896.png" alt="image-20221019194701896" style="zoom:50%;" /><img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221019194724575.png" alt="image-20221019194724575" style="zoom:50%;" />
2. Consider $F(w)=\frac{1}{4}w^2$. For each learning rate $\eta \in \{10^{-4},10^{-3},0.01,0.1,0.5,1,2,5,10,100\}$, calculate the sequence $\{w^t\}_{t=1}^{1000}$ generated by GD and plot the curve "$|w^t|$ v.s. $t$".
   - <img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221019194732734.png" alt="image-20221019194732734" style="zoom:50%;" /><img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221019194739317.png" alt="image-20221019194739317" style="zoom:50%;" />

# 4 Linear Regression

Suppose we are given a data set $\{\bold{x}_i,y_i\}_{i=1}^n$ where each $\bold{x}_i \in \mathbb{R}^d \times \mathbb{R}$ is a row vector. We hope to learn a mapping $f$ such that each $y_i$ is approximated by $f(\bold{x}_i)$. Then a popular approach is to fit the data with *linear regression* - it assumes there exists $\bold{w} \in \mathbb{R}^d$ such that $y_i \approx \bold{w} \cdot \bold{x}_i$. In order to learn $\bold{w}$ from the data, it typically boils down to solving the following *least-squares* program:
$$
min_{\bold{w} \in \mathbb{R}^d} F(\bold{w}):= \frac{1}{2}||\bold{y}-\bold{Xw}||^2, \qquad \qquad(4.1)
$$
where $\bold{X}$ is the data matrix with the $i^{th}$ row being $\bold{x}_i$, and $\bold{y}=(y_1,y_2,...,y_n)^\top$.

1. Compute the gradient and the Hessian matrix of $F(\bold{w})$â€‹, and show that (6.4) is a convex program.

   - $\frac{\partial F}{\partial \bold{w}}=-\bold{X}^{\top}||\bold{y}-\bold{Xw}||$, or
     $-\begin{bmatrix}
     \sum_{i=1}^{n}x_{i1}(y_i-\sum_{j=1}^{d}w_jx_{ij}) \\
     \vdots \\
     \sum_{i=1}^{n}x_{id}(y_i-\sum_{j=1}^{d}w_jx_{ij}) \\
     \end{bmatrix}$

   - $\nabla^2 F=\frac{\partial^2 F}{\partial \bold{w}^2}=\bold{X}^{\top}\bold{X}$, or $d \times n * n \times d $ 

     $\begin{bmatrix}\sum_{i=1}^nx_{i1}^2 & \dots & \sum_{i=1}^{n}x_{i1}x_{id} \\
     \vdots & \ddots & \vdots \\
     \sum_{i=1}^{n}x_{id}x_{i1} & \dots & \sum_{i=1}^{n}x_{id}^2\end{bmatrix}$

   - $F(\bold{w})$ *is convex* $\iff \nabla^2F(x) \ge 0, \forall x \in D$

     - Represent a (assumedly linearly independent) $\bold{X}$ as $\bold{X}=\begin{bmatrix}\bold{v_1}&\bold{v_2}&\dots&\bold{v_d}\end{bmatrix}$
       $k_1\bold{v_1}+\dots+k_d\bold{v_d}=\bold{0} \iff k_1=\dots=k_d=0$
       Now let $\bold{k}$ be an eigenvector of $\nabla^2F$,
       $\bold{k} \ne \bold{0} \implies (k_1\bold{v_1}+\dots+k_d\bold{v_d})^2>0$
     - $=[k_1 \dots k_d]
       \begin{bmatrix}
       \bold{v_1}\\
       \vdots\\
       \bold{v_d}
       \end{bmatrix}
       [\bold{v_1} \dots \bold{v_d}]
       \begin{bmatrix}
       k_1\\
       \vdots\\
       k_d
       \end{bmatrix}=\bold{k^{\top}}\nabla^2F\bold{k}=\lambda\bold{k^{\top}k}>0$
     - $\lambda\bold{k^{\top}k}>0 \rightarrow \bold{k^{\top}k}=\sum_{i=1}^dk_i^2>0 \implies \lambda>0$
       Since $k$ is arbitrary, all eigenvalues must be positive, and thus  $\nabla^2F$ is positive definite, which means it is also positive semi-definite.
     - Therefore, $F(\bold{w})$ is convex.

2. Note that (6.4) is equivalent to the following:
   $$
   min_{\bold{w} \in \mathbb{R}^d}||\bold{y}-\bold{Xw}||^{100},
   $$
   in the sense that any minimizer of (6.4) is also an optimum of the above, and vice versa. State why we stick with the least-squares formulation.

   - We stick with the least squares regression rather than least $n^{100}$ regression because least squares has the lowest sampling variance while maintaining the minimum variance among all linear unbiased estimators.

3. State when the objective function is strongly-convex and when it is not.

   - A Function is strongly convex if, for any $w_1,w_2$, $||\nabla F(w_2)-\nabla F(w_1)||_2 \ge \alpha ||w_2-w_1||_2$, where $\alpha$ is the min eigenvalue of $\nabla^2F(w)$.
   OLS is strongly convex if X is linearly independent.

4. Fix $n=1000$ and increase $d$ from 20 to 500, with a step size 20. For each problem size $(n,d)$, generate the data matrix $\bold{X} \in \mathbb{R}^{n \times d}$ and the response $y \in \mathbb{R}^n$, for example, using the python API `numpy.random.randn`. Then calculate the exact solution $w^*=(\bold{X}^{\top}\bold{X})^{-1}\bold{X}^{\top}\bold{y}$ of (6.4) and record the computation time. Plot the curve of "time v.s. d" and summarize your observation.

  - <img src="C:\Users\aughb\AppData\Roaming\Typora\typora-user-images\image-20221023152454085.png" alt="image-20221023152454085" style="zoom:50%;" />

5. Consider $n=100$ and $d=40$. Again, generate $\bold{X}$ and $y$, and calculate the optimal solution. Use python API to calculate the minimum and maximum eigenvalue of the Hessian matrix, and derive the upper bound on the learning rate $\eta$ in gradient descent (see the <nonexistant> slides for the bound). Let us denote this theoretical bound by $\eta_0$. Run GD on the data set with 6 choices of learning rate: $\eta \in \{0.01\eta_0, 0.1\eta_0, \eta_0, 2\eta_0, 20\eta_0, 100\eta_0\}$. Plot the curve of "$||w^t-w^*||$ v.s. $t$" for $1 \le t \le 100$ and summarize your observation. Note that you can start GD with $\bold{w^0}=\bold{0}$.
   - 
